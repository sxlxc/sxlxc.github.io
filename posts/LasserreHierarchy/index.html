<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="googlebot" content="noindex">
    <title>Understanding Lasserre Hierarchy</title>
    <link rel="stylesheet" href="../../css/fonts.css" />
    <link rel="stylesheet" href="../../css/default.css" />
    <link rel="stylesheet" href="../../css/pygentize.css" />
    <link rel="stylesheet" href="../../css/chao-theorems.css">
    <link rel="stylesheet" href="../../css/sidenotes.css">
    <script defer src="../../css/pangu.simple.js"></script>
    <script>
        // page title
        document.addEventListener("DOMContentLoaded", function () {
            const hostname = window.location.hostname;
            document.title = document.title + " | " + hostname;
        });

        // pangu
        document.addEventListener('DOMContentLoaded', () => {
            pangu.autoSpacingPage();
        });

        // mathjax
        MathJax = {
            options: {
                menuOptions: {
                    settings: {
                        enrich: false,        // true to enable semantic-enrichment
                        collapsible: false,   // true to enable collapsible math
                        speech: false,        // true to enable speech generation
                        braille: false,       // true to enable Braille generation
                        assistiveMml: false,  // true to enable assistive MathML
                    }
                },
                enableMenu: false
            },
            output: {
                font: 'mathjax-fira',
                fontPath: '/mathjax-fira-font'
            },
            tex: {
                macros: {
                    floor: ["{\\left\\lfloor #1 \\right\\rfloor}", 1],
                    ceil: ["{\\left\\lceil #1 \\right\\rceil}", 1],
                    set: ["{\\left\\{ #1 \\right\\}}", 1],
                    norm: ["{\\left\\| #1 \\right\\|}", 1],
                    F: "{\\mathbb F}",
                    R: "{\\mathbb R}",
                    C: "{\\mathbb C}",
                    Z: "{\\mathbb Z}",
                    e: "{\\varepsilon}",
                    mex: "\\mathop{\\operatorname{mex}}",
                    lcm: "\\mathop{\\operatorname{lcm}}",
                    dist: "\\mathop{\\operatorname{dist}}",
                    poly: "\\mathop{\\operatorname{poly}}",
                    polylog: "\\mathop{\\operatorname{polylog}}",
                    span: "\\mathop{\\operatorname{span}}",
                }
            }
        };
    </script>
    <script defer src="../../mathjax/tex-chtml.js"></script>

</head>

<body>
    <div class="navbar-space">
        
        <!-- A table of contents on the left side, but only for screens
                that are big enough -->
        <div id="contents-big">
            <p class="mini-header">Contents <a id="up-arrow" href="#">↑</a></p>
            <ul>
<li><a href="#ksubset-01n-to-kcap-set01n"><span class="math inline">\(K\subset [0,1]^n \to K\cap \set{0,1}^n\)</span></a></li>
<li><a href="#probability-perspective">Probability Perspective</a>
<ul>
<li><a href="#feasible-probability">Feasible Probability</a></li>
<li><a href="#projection-in-k">Projection in <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li><a href="#properties">Properties</a>
<ul>
<li><a href="#convex-hull-and-conditional-probability">Convex Hull and Conditional Probability</a></li>
<li><a href="#decomposition-theorem">Decomposition Theorem</a></li>
</ul></li>
<li><a href="#moment-relaxation">Moment Relaxation</a></li>
<li><a href="#applications">Applications</a>
<ul>
<li><a href="#sparsest-cut">Sparsest Cut</a></li>
<li><a href="#matching">Matching</a></li>
</ul></li>
<li><a href="#questions">Questions</a>
<ul>
<li><a href="#replace-m_tellysucceq-0-with-mathopmathrmlas_tprojyin-k">Replace <span class="math inline">\(M_t^\ell(y)\succeq 0\)</span> with <span class="math inline">\(\mathop{\mathrm{Las}}_t^{proj}(y)\in K\)</span></a></li>
<li><a href="#separation-oracle-for-implicit-k">Separation Oracle for Implicit <span class="math inline">\(K\)</span></a></li>
</ul></li>
</ul>
        </div>
        
    </div>
    <div class="text-space">
        <header class="no-print">
            <nav class="navbar">
                <a href="../../">Home</a>
                <div class="navright">
                    <a href="../../draft">Drafts</a>
                    <a href="../../about">About</a>
                </div>
            </nav>
        </header>

        <main role="main">
            <h1 class="pagetitle">Understanding Lasserre Hierarchy</h1>
            <article>
    <section class="subtitle">
        
            From a Probabilistic Perspective
        
        
    </section>
    <section class="header">
        Posted on June  5, 2025
        
            by Yu Cong and Hongjie Qing
        
    </section>
    <div class="info">
        
            Tags: <a title="All pages tagged 'optimization'." href="../../tags/optimization/index.html" rel="tag">optimization</a>, <a title="All pages tagged 'LP'." href="../../tags/LP/index.html" rel="tag">LP</a>
        
    </div>    
    <section>
        <p></p>
Useful links:
<ol type="1">
<li><a href="https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf" class="uri">https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf</a></li>
<li><a href="https://web.stanford.edu/class/cs369h/" class="uri">https://web.stanford.edu/class/cs369h/</a></li>
<li>Laurent’s survey <span class="citation" data-cites="laurent_comparison_2003">[<a href="#ref-laurent_comparison_2003" role="doc-biblioref">1</a>]</span></li>
<li><a href="https://rameesh-paul.github.io/sos.pdf" class="uri">https://rameesh-paul.github.io/sos.pdf</a></li>
<li><a href="https://www.ams.jhu.edu/~abasu9/papers/main-Lasserre.pdf" class="uri">https://www.ams.jhu.edu/~abasu9/papers/main-Lasserre.pdf</a></li>
<li>Chapter 3 of <a href="https://people.eecs.berkeley.edu/~venkatg/pubs/papers/thesis-ali-kemal-sinop.pdf">Ali
Kemal Sinop’s PhD thesis</a></li>
</ol>
<p></p>
When I started writing this post, I hadn’t found so many “useful links”
yet. The content below and link 1.2. only focus on using the Lasserre
hierarchy on LPs, while 3.4.5.6. mention more general cases.
<h1 data-number="1" id="ksubset-01n-to-kcap-set01n"><span class="header-section-number">1</span> <span class="math inline">\(K\subset [0,1]^n \to K\cap
\set{0,1}^n\)</span></h1>
<p></p>
We want to solve a 0-1 integer program. Since this task is NP-hard in
general, we usually consider its linear relaxation. Different LP
formulations have different integrality gaps. For example, consider the
following linear relaxation of the max matching IP in non-bipartite
graph.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\sum_{e\in \delta(v)} x(e)&amp;\leq 1   &amp;   &amp;\forall v\in V\\
                    x(e)&amp;\in [0,1]  &amp;   &amp;\forall e\in E
\end{aligned}
\end{equation*}\]</span> <code>:(</code>, this polytope is not integral.
Edmonds proved that the following formulation is integral.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\sum_{e\in \delta(v)} x(e)&amp;\leq 1   &amp;   &amp;\forall v\in V\\
                    x(e)&amp;\in [0,1]  &amp;   &amp;\forall e\in E\\
\sum_{e\in E[U]} x(e) &amp;\leq (|U|-1)/2 &amp;  &amp;\forall U\subset
V, |U| \text{ odd}
\end{aligned}
\end{equation*}\]</span>
<p></p>
Schrijver <span class="citation" data-cites="schrijver_polyhedral_1986">[<a href="#ref-schrijver_polyhedral_1986" role="doc-biblioref">2</a>]</span>
showed that those odd constraints can be obtained by adding cutting
planes to the previous polytope. Fortunately for matching polytope we
have a polynomial time separation oracle. However, for harder problems
adding cutting planes may make the program NP-hard to solve. Lasserre
hierarchy is a method to strengthen the polytope to approaching the
integer hull while providing provable good properties and keeping the
program polynomial time solvable (if applied constant number of times).
<h1 data-number="2" id="probability-perspective"><span class="header-section-number">2</span> Probability Perspective</h1>
<p></p>
There is a good interpretation of the linear relaxation of 0-1 integer
programs. Let <span class="math inline">\(K=\set{x\in \R^n| Ax\geq
b}\subset [0,1]^n\)</span> be the polytope of the linear relaxation. The
goal of solving the integer program is to describe all possible discrete
distribution over <span class="math inline">\(K\cap
\set{0,1}^n\)</span>. Note that for a fixed distribution the expected
position <span class="math inline">\((\sum_p
\mathop{\mathrm{Pr}}[X_p(1)=1] x_p(1),...,\sum_p
\mathop{\mathrm{Pr}}[X_p(n)=1] x_p(n))^T\)</span> is in <span class="math inline">\(\mathop{\mathrm{conv}}(K\cup \set{0,1}^n)\)</span>
and iterating over all possible distribution gives us the integer-hull.
Hence we can find the integral optimal solution if having access to all
distribution over integer points.
<p></p>
For any <span class="math inline">\(x\in K\)</span>, <span class="math inline">\(x_i\)</span> can be seen as the probability of
<span class="math inline">\(x_i=1\)</span>. We only care about the
discrete distribution on feasible integral points. However, each <span class="math inline">\(x\in K\)</span> only describes some marginal
probabilities and this this marginal probability may not be even
feasible. Consider the following 2D example. Any point in <span class="math inline">\(\text{green area}\setminus \text{orange
area}\)</span> is not a marginal distribution of any possible joint
distribution over <span class="math inline">\((0,0),(1,0)\)</span> and
<span class="math inline">\((1,1)\)</span>. The idea is to iteratively
prune this area.
<figure>
<img src="../images/lasserre/feasiblepoints.png" alt="2D example" style="width: 300px;display: block; margin: auto;" />
</figure>
<p></p>
Now we need to think about how to represent all possible joint
distribution. One natural way is to use a vector <span class="math inline">\(y\in \R^{2^n}\)</span> for the distribution law of
every possible integer point in <span class="math inline">\(\set{0,1}^n\)</span>. However, this method does
not work well with our existing marginal probabilities. Let <span class="math inline">\(y\in \R^{2^{n}}\)</span> be a random vector such
that <span class="math inline">\(y_I=\mathop{\mathrm{Pr}}[\bigwedge_{i\in
I}(x_i=1)]\)</span> and <span class="math inline">\(y_\emptyset=1\)</span>. Computing all feasible
<span class="math inline">\(y\)</span> is the same as finding all
possible bivariate discrete distribution on the integer points. To make
<span class="math inline">\(y\)</span> a feasible probability from some
joint distribution and to make <span class="math inline">\((y_{\set{1}},...,y_{\set{n}})^T\in K\)</span> we
have to add more constraints.
<!-- why psd? -->
<h2 data-number="2.1" id="feasible-probability"><span class="header-section-number">2.1</span> Feasible Probability</h2>
<p></p>
For now let’s forget about <span class="math inline">\(K\)</span> and
consider <span class="math inline">\(y\in [0,1]^{2^{[n]}}\)</span> and a
discrete distribution <span class="math inline">\(D\)</span> on <span class="math inline">\(\set{0,1}^n\)</span>. We want to make <span class="math inline">\(y_I=\mathop{\mathrm{Pr}}_{D}[\bigwedge_{i\in
I}(X_i=1)]\)</span>. In fact, there is a one to one correspondence
between <span class="math inline">\(y\)</span> and <span class="math inline">\(D\)</span>. If <span class="math inline">\(D\)</span> is given, computing <span class="math inline">\(y_I\)</span> is easy for any <span class="math inline">\(I\subseteq [n]\)</span>. If <span class="math inline">\(y\)</span> is given, recovering the distribution
<span class="math inline">\(D\)</span> is the same as solving a system
of <span class="math inline">\(2^n\)</span> linear equations with <span class="math inline">\(2^n\)</span> variables (<span class="math inline">\(2^n-1\)</span> of the the equations come form
<span class="math inline">\(y_I\)</span>, and the remaining one is <span class="math inline">\(\sum_p D(p)=1\)</span>.) Thus, with a slight abuse
of notation, we will refer to <span class="math inline">\(y\)</span> as
a distribution.
<p></p>
We work with the 2D example first. Let <span class="math inline">\(x=(x_1,x_2)^T\in K\)</span> be a marginal
distribution. One can see that <span class="math inline">\(y=(1,x_1,x_2,\mathop{\mathrm{Pr}}[X_1=X_2=1])^T\)</span>
and the last number is not arbitrary. In fact, <span class="math inline">\(\mathop{\mathrm{Pr}}[X_1=X_2=1]\)</span> must in
range <span class="math inline">\([\max(0,
x_1+x_2-1),\min(x_1,x_2)]\)</span>.
<p></p>
To make sure <span class="math inline">\(y\)</span> is indeed a
probability distribution the moment matrix is considered. The moment
matrix <span class="math inline">\(M(y)\)</span> is of size <span class="math inline">\(2^n \times 2^n\)</span> and <span class="math inline">\(M(y)[I,J]\)</span> is defined as the expectation
<span class="math inline">\(E[\prod_{i\in I\cup J}X_i]=y_{I\cup
J}\)</span> (the only non-zero term is <span class="math inline">\(1\cdot \mathop{\mathrm{Pr}}[\bigwedge_{i\in I\cup
j}(X_i=1)]=y_{I\cup J}\)</span>). The expectation is taken over the
distribution defined by <span class="math inline">\(y\)</span>.
<div class="theorem-environment Lemma" data-index="1" type="Lemma">
<span class="theorem-header"><span class="type">Lemma</span><span class="index">1</span></span>
<p></p>
For any probability distribution <span class="math inline">\(y\)</span>,
the moment matrix is psd.
</div>
<div class="theorem-environment Proof" type="Proof">
<span class="theorem-header"><span class="type">Proof</span></span>
<p></p>
We need to verify <span class="math inline">\(z^T M(y) z\geq 0\)</span>
for any <span class="math inline">\(z\)</span>. <span class="math display">\[\begin{equation*}
\begin{aligned}
z^T M(y) z    &amp;= \sum_I \sum_J z_I y_{I\cup J} z_J\\
                    &amp;= \sum_I \sum_J z_I E[\prod_{i\in I\cup J} X_i]
z_J\\
                    &amp;= E\left[\left( \sum_I (z_I \prod_{i\in I}
X_i)\right)^2 \right]
\end{aligned}
\end{equation*}\]</span>
</div>
<p></p>
Note that in the proof something like sum of squares appears. Lasserre
hierarchy has deep connections with <a href="https://en.wikipedia.org/wiki/Sum-of-squares_optimization">SOS
optimization</a> and is also known as sum-of-squares hierarchy.
<div class="theorem-environment Lemma" data-index="2" type="Lemma">
<span class="theorem-header"><span class="type">Lemma</span><span class="index">2</span></span>
<p></p>
If <span class="math inline">\(M(y)\)</span> is psd then <span class="math inline">\(y\)</span> is a probability distribution.
</div>
It is easy to see that <span class="math inline">\(y_I\in [0,1]\)</span>
for all <span class="math inline">\(I\subseteq [n]\)</span>. Consider
the following submatrix
<span class="math display">\[\begin{bmatrix}
\emptyset &amp; y_I\\
y_I &amp; y_I
\end{bmatrix}\]</span>
<p></p>
It is psd since <span class="math inline">\(M(y)\)</span> is psd. The
determinant is <span class="math inline">\(y_I(1-y_I)\geq 0\)</span>.
<p></p>
Let <span class="math inline">\(\mathop{\mathrm{Pr}}_D[p]\)</span> be
the probability of selecting <span class="math inline">\(p\in\set{0,1}^n\)</span> in <span class="math inline">\(D\)</span>. It remains to prove the following
system of linear equations has a solution such that <span class="math inline">\(\mathop{\mathrm{Pr}}_D[p]\in [0,1]\)</span> for
all <span class="math inline">\(p\)</span>.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
y_{[n]} &amp;=  \mathop{\mathrm{Pr}}_D[\mathbf 1]\\
y_{[n]\setminus \set{n}} &amp;= \sum_{p:\bigwedge\limits_{i\in
[n-1]}(p_i=1)} \mathop{\mathrm{Pr}}_D[p]\\
y_{[n]\setminus \set{n-1}} &amp;= \sum_{p:\bigwedge\limits_{i\in
[n]\setminus \set{n-1}}(p_i=1)} \mathop{\mathrm{Pr}}_D[p]\\
        &amp;\vdots             \\
y_{\set{1}} &amp;= \sum_{p:p_1=1} \mathop{\mathrm{Pr}}_D[p]\\
y_\emptyset &amp;= \sum_p \mathop{\mathrm{Pr}}_D[p]
\end{aligned}
\end{equation*}\]</span> I believe this can be proven with the idea of
Lemma 2 <a href="https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf">here</a>.
<!-- slacks? -->
<h2 data-number="2.2" id="projection-in-k"><span class="header-section-number">2.2</span> Projection in <span class="math inline">\(K\)</span></h2>
<p></p>
Let the projection of <span class="math inline">\(y\)</span> be <span class="math inline">\((y_{\set{1}},\dots,y_{\set{n}})^T\)</span>. For
any <span class="math inline">\(y\)</span> the projection should always
lie in <span class="math inline">\(K\)</span>. One may want to define
moment matrices for constraints <span class="math inline">\(Ax\geq
b\)</span>. This is called the moment matrix of slacks. For simplicity
we only consider one linear constraint <span class="math inline">\(a^Tx-b\geq 0\)</span>. The moment matrix for this
constraint is <span class="math inline">\(M(y)=\left( \sum_{i=1}^n a_i
y_{I\cup J\cup \set{i}}-b y_{I\cup J} \right)_{I,J\subseteq
[n]}\)</span>. Then we can do similar arguments.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
z^T M(y) z    &amp;= \sum_I \sum_J z_I z_J (\sum_{i=1}^n a_i y_{I\cup
J\cup \set{i}}-b y_{I\cup J})\\
                    &amp;= \sum_I \sum_J z_I z_J (\sum_i a_i
E[\prod_{k\in I\cup J\cup\set{i}} X_k] - b E[\prod_{k\in I\cup J}X_k]
)\\
                    &amp;= E\left[ \sum_I \sum_J z_I z_J (\sum_i a_i X_i
-b) \prod_{k\in I\cup J}X_k \right]\\
                    &amp;= E\left[ (\sum_i a_i X_i -b) \left(\sum_I z_I
\prod_{i\in I} X_i \right)^2 \right]
\end{aligned}
\end{equation*}\]</span>
<p></p>
Note that we can combine the expectations since they are taken over the
same probability distribution. Now assume that we have <span class="math inline">\(a^TX-b\geq 0\)</span>.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
E&amp;\left[ (\sum_i a_i X_i -b) \left(\sum_I z_I \prod_{i\in I} X_i
\right)^2 \right]\\
    &amp;= \sum \mathop{\mathrm{Pr}}[\cdots](a^T X-b)\left(\sum_I z_I
\prod_{i\in I} X_i \right)^2 \geq 0
\end{aligned}
\end{equation*}\]</span>
<p></p>
If <span class="math inline">\(a^TX\geq b\)</span> is satisfied, then
the corresponding slack moment matrix is psd.
<p></p>
Finally, this is a more formal definiton.
<div class="theorem-environment Definition" data-index="3" type="Definition" title="$t$-th level of Lasserre hierarchy">
<span class="theorem-header"><span class="type">Definition</span><span class="index">3</span><span class="name"><span class="math inline">\(t\)</span>-th level of Lasserre
hierarchy</span></span>
<p></p>
The <span class="math inline">\(t\)</span>-th level of Lasserre
hierarchy <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span> of a convex
polytope <span class="math inline">\(K=\set{x\in \R^n| Ax\geq b}\subset
[0,1]^n\)</span> is the set of vectors <span class="math inline">\(y\in
\R^{2^n}\)</span> that make the following matrices psd.
<ol type="1">
<li>moment matrix <span class="math inline">\(M_t(y):=(y_{I\cup
J})_{|I|,|J|\leq t}\succeq 0\)</span></li>
<li>moment matrix of slacks <span class="math inline">\(M_t^\ell(y):=\left( \sum_{i=1}^n A_{\ell
i}y_{I\cup J\cup \set{i}}-b_\ell y_{I\cup J} \right)_{|I|,|J|\leq
t}\succeq 0\)</span></li>
</ol>
</div>
<p></p>
Note that the <span class="math inline">\(t\)</span>-th level of
Lasserre hierarchy only involve entries <span class="math inline">\(y_I\)</span> with <span class="math inline">\(|I|\leq 2t+1\)</span>. (<span class="math inline">\(+1\)</span> comes from the moment matrix of
slacks) The matrices have dimension <span class="math inline">\(\binom{n}{2t+1}=n^{O(t)}\)</span> and there are
only <span class="math inline">\(m+1\)</span> matrices. Thus to optimize
some objective over the <span class="math inline">\(t\)</span>-th level
of Lasserre hierarchy takes <span class="math inline">\(mn^{O(t)}\)</span> time which is still polynomial
in the input size. (The separation oracle computes eigenvalues and
eigenvectors. If there is a negative eigenvalue we find the
corresponding eigenvector <span class="math inline">\(v\)</span> and the
separating hyperplane is <span class="math inline">\(\sum_{I,J}v_{I}v_{J} x_{I,J}=0\)</span>. See <a href="https://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec17.pdf">Example
43</a>.)
<h1 data-number="3" id="properties"><span class="header-section-number">3</span> Properties</h1>
<p></p>
Almost everything in this part can be found <a href="https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf">here</a>.
<p></p>
Suppose that we have the <span class="math inline">\(t\)</span>-th level
of Lasserre hierarchy <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span>. Denote by
<span class="math inline">\(\mathop{\mathrm{Las}}_t^{proj}(K)\)</span>
the projection of the <span class="math inline">\(t\)</span>-th level.
<ol start="0" type="1">
<li><span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span> is
convex</li>
<li><span class="math inline">\(y_I\in [0,1]\)</span> for all <span class="math inline">\(y\in \mathop{\mathrm{Las}}_t(K)\)</span></li>
<li><span class="math inline">\(0\leq y_I \leq y_J \leq 1\)</span> for
all <span class="math inline">\(J\subset I\)</span> with <span class="math inline">\(|I|,|J|\leq t\)</span></li>
<li><span class="math inline">\(y_{I\cup J}\leq \sqrt{y_I \cdot
y_J}\)</span></li>
<li><span class="math inline">\(K\cap \set{0,1}^n \subset
\mathop{\mathrm{Las}}_t^{proj}(K)\)</span> for all <span class="math inline">\(t\in [n]\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Las}}_t^{proj}(K)\subset
K\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Las}}_n(K)\subset
\mathop{\mathrm{Las}}_{n-1}(K)\subset \dots \subset
\mathop{\mathrm{Las}}_0(K)\)</span></li>
</ol>
<p></p>
1.2.3. show that <span class="math inline">\(y\)</span> behaves
similarly to a real probability distribution.
<p></p>
4.5.6. show that <span class="math inline">\(K\cap \set{0,1}^n \subset
\mathop{\mathrm{Las}}_n^{proj}(K)\subset
\mathop{\mathrm{Las}}_{n-1}^{proj}(K)\subset \dots \subset
\mathop{\mathrm{Las}}_0^{proj}(K) = K\)</span>.
<p></p>
The goal of this section is to show that <span class="math inline">\(K\cap \set{0,1}^n =
\mathop{\mathrm{Las}}_n^{proj}(K)\)</span>. When working on the Lasserre
hierarchy, instead of considering the projection <span class="math inline">\(x_i\)</span> solely, we usually perform the
analysis on <span class="math inline">\(y\)</span>.
<h2 data-number="3.1" id="convex-hull-and-conditional-probability"><span class="header-section-number">3.1</span> Convex Hull and Conditional
Probability</h2>
<div id="conv" class="theorem-environment Lemma" data-index="4" type="Lemma">
<span class="theorem-header"><span class="type">Lemma</span><span class="index">4</span></span>
<p></p>
For <span class="math inline">\(t\geq 1\)</span>, let <span class="math inline">\(y\in \mathop{\mathrm{Las}}_t(K)\)</span> and <span class="math inline">\(S\subset [n]\)</span> be any subset of variables
of size at most <span class="math inline">\(t\)</span>. then <span class="math display">\[y\in \mathop{\mathrm{conv}}\set{z\in
\mathop{\mathrm{Las}}_{t-|S|}(K)| z_i\in \set{0,1} \forall i\in
S}.\]</span>
</div>
<p></p>
For any <span class="math inline">\(y\in\mathop{\mathrm{Las}}_n(K)\)</span> and <span class="math inline">\(S=[n]\)</span>, the previous lemma implies the
projection of <span class="math inline">\(y\)</span> is convex
combination of integral vectors in <span class="math inline">\(K\cap
\set{0,1}^n\)</span>. Then it follows that <span class="math inline">\(\mathop{\mathrm{Las}}_n^{proj}(K)=K\cap
\set{0,1}^n\)</span>. This also provides proofs for the facts that if
<span class="math inline">\(M_n(y)\succeq 0\)</span> and <span class="math inline">\(M_n^{\ell}(y)\succeq 0\)</span> then <span class="math inline">\(y\)</span> is indeed a probability distribution
and the projection is in <span class="math inline">\(K\)</span>.
<div class="theorem-environment Proof" type="Proof">
<span class="theorem-header"><span class="type">Proof</span></span>
<p></p>
The proof is constructive and is by induction on the size of <span class="math inline">\(S\)</span>.
<ul>
<li><span class="math inline">\(S=\set{i}\)</span>. Assume that <span class="math inline">\(y_{\set{i}}\in (0,1)\)</span>. For simplicity I
use <span class="math inline">\(y_i\)</span> for <span class="math inline">\(y_{\set{i}}\)</span>. Define two vectors <span class="math inline">\(z^{(1)},z^{(2)}\)</span> as <span class="math inline">\(z^{(1)}_I=\frac{y_{I\cup\set{i}}}{y_i}\)</span>
and <span class="math inline">\(z^{(2)}_I=\frac{y_I-y_{I\cup\set{i}}}{1-y_i}\)</span>.
One can easily verify that <span class="math inline">\(y=y_i
z^{(1)}+(1-y_i)z^{(2)}, z^{(1)}_i=1\)</span> and <span class="math inline">\(z^{(2)}_i=0\)</span>. It remains to verify <span class="math inline">\(z^{(1)},z^{(2)}\in
\mathop{\mathrm{Las}}_{t-1}(K)\)</span>. Since <span class="math inline">\(M_t(y)\)</span> is psd, there must be vectors
<span class="math inline">\(v_I,v_J\)</span> such that <span class="math inline">\(\langle v_I,v_J \rangle=y_{I\cup J}\)</span> for
all <span class="math inline">\(|I|,|J|\leq t\)</span>. Take <span class="math inline">\(v_I^{(1)}=v_{I\cup\set{i}}/\sqrt{y_i}\)</span>. We
have <span class="math display">\[\langle v_I^{(1)},v_J^{(1)}
\rangle=\frac{y_{I\cup
J\cup\set{i}}}{y_i}=M_{t-1}(z^{(1)})[I,J]\]</span> for all <span class="math inline">\(|I|,|J|\leq t-1\)</span>. Thus <span class="math inline">\(M_{t-1}(z^{(1)})\)</span> is psd. Similarly, one
can take <span class="math inline">\(v_I^{(2)}=(v_I-v_{I\cup
\set{i}})/\sqrt{(1-y_i)}\)</span> and show <span class="math inline">\(M_{t-1}(z^{(2)})\)</span> is psd. <br> For each
moment matrix of slacks one can use exactly the same arguments to show
<span class="math inline">\(M_{t-1}^{\ell}(z^{(1)})\succeq 0\)</span>
and <span class="math inline">\(M_{t-1}^{\ell}(z^{(2)})\succeq
0\)</span>.</li>
<li>For the inductive steps one can see that our arguments for the base
case can be applied recursively on <span class="math inline">\(z^{(1)},z^{(2)}\)</span>.</li>
</ul>
</div>
<p></p>
<span class="math inline">\(y\in \mathop{\mathrm{Las}}_t(K)\)</span> is
a probability distribution if we consider only <span class="math inline">\(|I|\leq t\)</span>, <span class="math inline">\(y_I=\mathop{\mathrm{Pr}}[\bigwedge_{i\in
I}X_i=1]\)</span>. The vectors <span class="math inline">\(z^{(1)},z^{(2)}\)</span> we constructed in the
previous proof can be understood as conditional probabilities. <span class="math display">\[\begin{equation*}
\begin{aligned}
&amp;z^{(1)}_I=\frac{y_{I\cup\set{i}}}{y_i}=\frac{\mathop{\mathrm{Pr}}[\bigwedge_{k\in
I\cup
\set{i}}X_k=1]}{\mathop{\mathrm{Pr}}[X_i=1]}=\mathop{\mathrm{Pr}}[\bigwedge_{k\in
I}X_k=1 | X_i=1]\\
&amp;z^{(2)}_I=\frac{y_I-y_{I\cup\set{i}}}{1-y_i}=\frac{\mathop{\mathrm{Pr}}[\bigwedge_{k\in
I} (X_k=1) \land
X_i=0]}{\mathop{\mathrm{Pr}}[X_i=0]}=\mathop{\mathrm{Pr}}[\bigwedge_{k\in
I}X_k=1 | X_i=0]
\end{aligned}
\end{equation*}\]</span>
<p></p>
The proof is basically showing that
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
y_I &amp;= \mathop{\mathrm{Pr}}[X_i=1]
\mathop{\mathrm{Pr}}[\bigwedge_{k\in I}X_k=1 |
X_i=1]+\mathop{\mathrm{Pr}}[X_i=0] \mathop{\mathrm{Pr}}[\bigwedge_{k\in
I}X_k=1 | X_i=0]\\
    &amp;= \mathop{\mathrm{Pr}}[\bigwedge_{i\in I}X_i=1]
\end{aligned}
\end{equation*}\]</span>
<p></p>
For any partially feasible probability distribution <span class="math inline">\(y\in\mathop{\mathrm{Las}}_t(K)\)</span>, <span class="math inline">\(y_i \in (0,1)\)</span> implies that both <span class="math inline">\(X_i=0\)</span> and <span class="math inline">\(X_i=1\)</span> happen with non-zero probability,
which in turn impies <span class="math inline">\(z^{(1)},z^{(2)}\in
\mathop{\mathrm{Las}}_{t-1}(K)\)</span>. One can also explicitly express
<span class="math inline">\(y\)</span> as convex combination and see the
relation with Möbius inversion, see p9 in <a href="https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf">this
notes</a>.
<p></p>
In <a href="#conv" title="Lemma 4">Lemma 4</a>, each vector in the
convex combination (those with integer value on <span class="math inline">\(S\)</span>, such as <span class="math inline">\(z^{(1)},z^{(2)}\)</span>) can be understood as a
partial probability distribution under condition <span class="math inline">\([\bigwedge_{i\in I} (X_i=1) \bigwedge_{j\in
J}(X_j=0)]\)</span> where <span class="math inline">\(I\sqcup
J=S\)</span>, and the probability assigned to it is exactly the chance
its condition happens. More formally, <a href="#conv" title="Lemma 4">Lemma 4</a> implies the following,
<div class="theorem-environment Corollary" data-index="5" type="Corollary">
<span class="theorem-header"><span class="type">Corollary</span><span class="index">5</span></span>
<p></p>
Let <span class="math inline">\(y\in\mathop{\mathrm{Las}}_t(K)\)</span>.
For any subset <span class="math inline">\(S\subset [n]\)</span> of size
at most <span class="math inline">\(t\)</span>, there is a distribution
<span class="math inline">\(D(S)\)</span> over <span class="math inline">\(\set{0,1}^S\)</span> such <span class="math display">\[
\mathop{\mathrm{Pr}}_{z\sim D(S)}\left[ \bigwedge_{i\in I} (z_i=1)
\right]=y_I \quad \forall I\subset S
\]</span>
</div>
<p></p>
Moreover, this distribution is “locally consistent” since the
prabability assigned to each vector only depends on its condition.
<p></p>
Since the constraints in <span class="math inline">\(\mathop{\mathrm{Las}}_t\)</span> only concern the
psdness of certain matrices, one may naturally think about its
decomposition. This leads to a vector representation of <span class="math inline">\(y_I\)</span> for all <span class="math inline">\(|I|\leq t\)</span> and may be helpful in rounding
algorithms. For <span class="math inline">\(J\subset I\)</span>, <span class="math inline">\(v_I\)</span> lies on the sphere of radius <span class="math inline">\(\|v_J\|/2=\sqrt{y_J}/2\)</span> and center <span class="math inline">\(v_J /2\)</span>.
<h2 data-number="3.2" id="decomposition-theorem"><span class="header-section-number">3.2</span> Decomposition Theorem</h2>
<p></p>
We have seen that <span class="math inline">\(\mathop{\mathrm{Las}}_n^{proj}(K)\)</span> is the
integer hull. Can we get better upperbounds based on properties of <span class="math inline">\(K\)</span>? Another easy upperbound is <span class="math inline">\(\max_{x\in
K}|\mathop{\mathrm{ones}}(x)|+1\)</span>, where <span class="math inline">\(\mathop{\mathrm{ones}}(x)=\set{i|x_i=1}\)</span>.
This is because <span class="math inline">\(y\in
\mathop{\mathrm{Las}}_t(K)\)</span> is a partial distribution for <span class="math inline">\(|I|\leq t\)</span> that can be realized as the
marginal distribution of some distribution on <span class="math inline">\(K\cap \set{0,1}^n\)</span>; if <span class="math inline">\(k\cap \set{0,1}^n\)</span> does not contain a
point with at least <span class="math inline">\(t\)</span> ones, we
certainly have <span class="math inline">\(\mathop{\mathrm{Pr}}[\bigwedge_{i\in
I}(X_i=1)]=0\)</span> for <span class="math inline">\(|I|\geq
t\)</span>.
<p></p>
This fact implies that for most hard problems we should not expect <span class="math inline">\(\mathop{\mathrm{Las}}_k\)</span> to give us a
integral solution for constant <span class="math inline">\(k\)</span>.
<p></p>
Karlin, Mathieu and Nguyen <span class="citation" data-cites="karlin_integrality_2011">[<a href="#ref-karlin_integrality_2011" role="doc-biblioref">3</a>]</span>
proved a more general form of <a href="#conv" title="Lemma 4">Lemma
4</a> using similar arguments.
<div class="theorem-environment Theorem" data-index="6" type="Theorem" title="Decomposition Theorem">
<span class="theorem-header"><span class="type">Theorem</span><span class="index">6</span><span class="name">Decomposition
Theorem</span></span>
<p></p>
Let <span class="math inline">\(y\in
\mathop{\mathrm{Las}}_t(K)\)</span>, <span class="math inline">\(S\subset [n]\)</span> and <span class="math inline">\(k\in [0,t]\)</span> such that <span class="math inline">\(k\geq |\mathop{\mathrm{ones}}(x)\cap S|\)</span>
for all <span class="math inline">\(x\in K\)</span>. Then <span class="math display">\[
y\in \mathop{\mathrm{conv}}\set{z| z\in \mathop{\mathrm{Las}}_{t-k}(K);
z_{\set{i}}\in \set{0,1} \forall i\in S}.
\]</span>
</div>
<h1 data-number="4" id="moment-relaxation"><span class="header-section-number">4</span> Moment Relaxation</h1>
<p></p>
In this section we briefly show the non-probabilistic view of Lasserre
hierarchy and how this idea is used in polynomial optimization problems.
<p></p>
Everything in this section can be found in <a href="https://people.eecs.berkeley.edu/~venkatg/pubs/papers/thesis-ali-kemal-sinop.pdf"><code>useful_link[6]</code></a>.
<p></p>
Consider the following polynomial optimiation problem <span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   a(x)&amp;    &amp;   &amp;\\
s.t.&amp;   &amp;   b(x)&amp;\geq 0 &amp;   &amp;\forall b\in B\\
    &amp;   &amp;   x&amp;\in\set{0,1}^n
\end{aligned}
\end{equation*}\]</span> where <span class="math inline">\(a,b,c\)</span> are polynomials. We want to
formulate this problem with SDP.
<p></p>
We can consider polynomials <span class="math inline">\(a,b\)</span> as
multilinear polynomials. Since <span class="math inline">\(x_i\in
\set{0,1}\)</span>, we have <span class="math inline">\(x_i^2=x_i\)</span>. Now we can consider
enumerating <span class="math inline">\(x_S=\prod_{i\in S}x_i\)</span>
and write these polynomials as linear functions. For example, we can
rewrite <span class="math inline">\(a(x)=\sum_{S\subset
[n]}\sum_{\alpha_S:S\to \Z} a_S \prod_{i\in S}x_i^{\alpha_S(i)}\)</span>
as <span class="math inline">\(\sum_{S\subset [n]} a_S x_S\)</span>
which is linear in the moment sequence <span class="math inline">\((x_\emptyset,
x_{\set{1}},\ldots,x_{[n]})\)</span>.
<p></p>
Recall that our goal is to find a SDP formulation. A common technique is
replace each variable with a vector. We consider the moment vectors
<span class="math inline">\([v_S\in \R^\gamma]_{S\in 2^{[n]}}\)</span>.
Similar to the LP case, we want <span class="math inline">\(\langle
v_A,v_B \rangle=x_{A\cup B}\)</span>. This is exactly the Gram
decomposition of the moment matrix. There exist such moment vectors iff
the moment matrix is psd. For <span class="math inline">\(b(x)\geq
0\)</span>, we consider the slack moment matrix <span class="math inline">\(M^b(x)=\left( \sum_S b_S x_{I\cup J\cup S}
\right)_{I,J}\)</span>
<p></p>
Then the program becomes the following SDP
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   \sum_{S\subseteq [n]}a_S x_S&amp;    &amp;   &amp;\\
s.t.&amp;   &amp;   M^b(x)&amp;\succeq 0 &amp;   &amp;\forall b\in B\\
    &amp;   &amp;   M(x)&amp;\succeq 0\\
    &amp;   &amp;   x_{\emptyset}&amp;=1
\end{aligned}
\end{equation*}\]</span>
<p></p>
Note that if the max degree of polynomials <span class="math inline">\(a,b\)</span> is at most <span class="math inline">\(d\)</span>, then the following program is a
relaxation of the original polynomial optimiation problem (cf. <a href="https://people.eecs.berkeley.edu/~venkatg/pubs/papers/thesis-ali-kemal-sinop.pdf">Corollary
3.2.2.</a>).
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   \sum_{S\subseteq [n]}a_S x_S&amp;    &amp;   &amp;\\
s.t.&amp;   &amp;   M_{F}^b(x)&amp;\succeq 0 &amp;   &amp;\forall b\in
B\\
    &amp;   &amp;   M_{F\uplus V_{\leq d}}(x)&amp;\succeq 0\\
    &amp;   &amp;   x_{\emptyset}&amp;=1
\end{aligned}
\end{equation*}\]</span> where <span class="math inline">\(F\subset
2^{[n]}\)</span>, <span class="math inline">\(\uplus(A,B)=\set{a\cup b|
\forall a\in A,b\in B}\)</span> is element-wise union and <span class="math inline">\(M_{F}\)</span> is the submatrix of <span class="math inline">\(M(F)\)</span> on entries <span class="math inline">\(F\times F\)</span>. Taking <span class="math inline">\(F=\binom{[n]}{\leq t}\)</span> gives us <span class="math inline">\(\mathop{\mathrm{Las}}_t\)</span>.
<h1 data-number="5" id="applications"><span class="header-section-number">5</span> Applications</h1>
<h2 data-number="5.1" id="sparsest-cut"><span class="header-section-number">5.1</span> Sparsest Cut</h2>
<p></p>
There are lots of applications in the useful links, but none of them
discusses sparsest cut <span class="citation" data-cites="guruswami_approximating_2013">[<a href="#ref-guruswami_approximating_2013" role="doc-biblioref">4</a>]</span>.
<div class="theorem-environment Problem" data-index="7" type="Problem" title="sparsest cut">
<span class="theorem-header"><span class="type">Problem</span><span class="index">7</span><span class="name">sparsest cut</span></span>
<p></p>
Given a vertex set <span class="math inline">\(V\)</span> and two weight
functions <span class="math inline">\(c,D:\binom{V}{2} \to \R_{\geq
0}\)</span>, find <span class="math inline">\(T\subset V\)</span> that
minimizes the sparsity of <span class="math inline">\(T\)</span> <span class="math display">\[
\Phi(T)=\frac{\sum_{u &lt; v}c_{u,v}|\chi^T(u)-\chi^T(v)|}{\sum_{u &lt;
v}D_{u,v}|\chi^T(u)-\chi^T(v)|},
\]</span> where <span class="math inline">\(\chi^T\)</span> is the
indicator vector of <span class="math inline">\(T\)</span>.
</div>
<p></p>
In <span class="citation" data-cites="guruswami_approximating_2013">[<a href="#ref-guruswami_approximating_2013" role="doc-biblioref">4</a>]</span> Guruswami and Sinop describe Lasserre
hierarchy in a slightly different way. (Note that <a href="https://people.eecs.berkeley.edu/~venkatg/pubs/papers/thesis-ali-kemal-sinop.pdf"><code>useful_link[6]</code></a>
is Sinop’s thesis) We have seen that <span class="math inline">\(y\in
[0,1]^{2^{[n]}}\)</span> is sufficient for describing the joint
distribution. However, the total number of events is <span class="math inline">\(3^n\)</span>, since for each variable <span class="math inline">\(X_i\)</span> in an event there are 3 possible
states, <span class="math inline">\(X_i=0,X_i=1\)</span> and <span class="math inline">\(X_i\)</span> is absent.
<p></p>
Instead of using <span class="math inline">\(y\in
[0,1]^{2^{[n]}}\)</span>, they enumerate each of the <span class="math inline">\(3^n\)</span> events and consider the vectors in
the Gram decomposition. For each set <span class="math inline">\(S\subset V\)</span> of size <span class="math inline">\(\leq r+1\)</span>, and for each 0-1 labeling <span class="math inline">\(f\)</span> on elements of <span class="math inline">\(S\)</span>, they define a vector <span class="math inline">\(x_S(f)\)</span>. Note that <span class="math inline">\(S(f)\)</span> enumerates all events and one should
understand <span class="math inline">\(x_S(f)\)</span> as the vector
corresponding to <span class="math inline">\(y_{S,f}\in
[0,1]^{3^{[n]}}\)</span> in the Gram decomposition and <span class="math inline">\(\langle x_S(f), x_T(g) \rangle=y_{f(S)\land
g(T)}\)</span>. Then <span class="math inline">\(x_S(f)\)</span> should
have the following properties:
<ol type="1">
<li>if <span class="math inline">\(f(S)\)</span> and <span class="math inline">\(g(T)\)</span> are inconsistant, i.e. there is an
element <span class="math inline">\(e\in S\cap T\)</span> and <span class="math inline">\(f(e)\neq g(e)\)</span>, then one should have <span class="math inline">\(\langle x_S(f), x_T(g) \rangle=y_{f(S)\land
g(T)}=0\)</span>.</li>
<li>if <span class="math inline">\(f(S)\land g(T)\)</span> and <span class="math inline">\(f'(A)\land g'(B)\)</span> are the same
event, i.e. <span class="math inline">\(A\cup B=S\cup T\)</span> and the
labels are the same, then <span class="math inline">\(\langle x_S(f),
x_T(g) \rangle=\langle x_A(f'), x_B(g') \rangle\)</span></li>
<li><span class="math inline">\(\|x_{\emptyset}\|^2=1\)</span> here
<span class="math inline">\(\emptyset\)</span> is the union of all
events.</li>
<li>for all <span class="math inline">\(u\in V\)</span>, <span class="math inline">\(\|x_u(0)\|^2+\|x_u(1)\|^2=\|x_{\emptyset}\|^2=1\)</span>.</li>
<li>for <span class="math inline">\(S\subset V, u\in S\)</span> and
<span class="math inline">\(f\in \set{0,1}^{S\setminus
\set{u}}\)</span>, <span class="math inline">\(x_S(f\land
(u=1))+x_S(f\land (u=0))=x_{S\setminus \set{u}}(f)\)</span>. (Note that
two lhs vectors are orthogonal)</li>
</ol>
<div id="pseudoPr" class="theorem-environment Lemma" data-index="8" type="Lemma" title="pseudo probability">
<span class="theorem-header"><span class="type">Lemma</span><span class="index">8</span><span class="name">pseudo
probability</span></span>
<p></p>
Let <span class="math inline">\(x\in \mathop{\mathrm{Las}}_t(V)\)</span>
for <span class="math inline">\(t\geq 0\)</span>. Then the following
holds:
<ol type="1">
<li><span class="math inline">\(\|x_S(f)\|^2 \in [0,1]\)</span> for all
<span class="math inline">\(|S|\leq t+1\)</span>.</li>
<li><span class="math inline">\(\|x_S(f)\|^2 \leq \|x_T(g)\|^2\)</span>
if <span class="math inline">\(T\subset S\)</span> and <span class="math inline">\(f(t)=g(t)\)</span> for all <span class="math inline">\(t\in T\)</span>.</li>
<li><span class="math inline">\(\|x_S(f)\|^2 = \sum_{h\in
\set{0,1}^{T-S}} \|x_T(f\land h)\|^2\)</span> if <span class="math inline">\(S\subset T\)</span>.</li>
<li>If <span class="math inline">\(S\in \binom{V}{\leq t}\)</span>,
<span class="math inline">\(f\in \set{0,1}^S\)</span> and <span class="math inline">\(u\notin S\)</span>, then <span class="math inline">\(x_{S+u}(f\land u=1)+x_{S+u}(f\land
u=0)=x_{S}(f)\)</span>.</li>
</ol>
</div>
<div class="theorem-environment Proof" type="Proof">
<span class="theorem-header"><span class="type">Proof</span></span>
<p></p>
Let <span class="math inline">\(N_t=\sum_{r=0}^{t+1}\binom{V}{r}2^r\)</span> be
the number of vectors in <span class="math inline">\(x\)</span>.
Consider the moment matrix <span class="math inline">\(M_t\in
\R^{N_t\times N_t}\)</span>, where each entry <span class="math inline">\(M_t[f(S),g(T)]\)</span> is <span class="math inline">\(\langle x_S(f),x_T(g)\rangle\)</span>. The moment
matrix is positive semidefinite since vectors in <span class="math inline">\(x\)</span> form a Gram decomposition of <span class="math inline">\(M_t\)</span>.
<ol type="1">
<li>Consider the following submatrix of <span class="math inline">\(M_t\)</span>. <span class="math display">\[\begin{bmatrix}
\langle x_\emptyset,x_\emptyset\rangle    &amp; \langle
x_\emptyset,x_S(f)\rangle\\
\langle x_S(f),x_\emptyset\rangle         &amp; \langle
x_S(f),x_S(f)\rangle
\end{bmatrix}\succeq 0\]</span> Computing the determinant gives us <span class="math inline">\(\|x_S(f)\|^2(1-\|x_S(f)\|^2)\geq 0\)</span>.</li>
<li>Again consider certain submatrix of <span class="math inline">\(M_t\)</span>. <span class="math display">\[\begin{bmatrix}
\langle{x_T(g)},{x_T(g)}\rangle  &amp; \langle{x_T(g)},{x_S(f)}\rangle\\
\langle{x_S(f)},{x_T(g)}\rangle  &amp; \langle{x_S(f)},{x_S(f)}\rangle
\end{bmatrix}\succeq 0\]</span> The determinant is <span class="math inline">\(\|x_S(f)\|^2(\|x_T(g)\|^2-\|x_S(f)\|^2)\geq
0\)</span>.</li>
<li>We only need to show <span class="math inline">\(\|x_S(f)\|^2=\|x_{S+u}(f\land u=0)\|^2
+\|x_{S+u}(f\land u=1)\|^2\)</span> and the rest follows by induction.
Note that <span class="math inline">\(x_u(0)+x_u(1)=x_\emptyset\)</span>
since we have <span class="math inline">\(\|x_u(0)\|^2+\|x_u(1)\|^2=\|x_{\emptyset}\|^2\)</span>
and they are orthogonal. <span class="math display">\[\begin{equation*}
\begin{aligned}
\|x_{S+u}(f\land u=0)\|^2 +\|x_{S+u}(f\land u=1)\|^2 &amp;=
\langle{x_S(f)},{x_u(0)}\rangle+\langle{x_S(f)},{x_u(1)}\rangle\\
&amp;= \langle{x_S(f)},{x_u(0)+x_u(1)}\rangle\\
&amp;= \langle{x_S(f)},{x_\emptyset}\rangle=\|x_S(f)\|^2
\end{aligned}
\end{equation*}\]</span></li>
<li>Notice that <span class="math inline">\(x_{S+u}(f\land u=1)\)</span>
and <span class="math inline">\(x_{S+u}(f\land u=0)\)</span> are
orthogonal. Denote by <span class="math inline">\(x_S(f')\)</span>
the projection of <span class="math inline">\(f\)</span> on the
hyperplane spanned by <span class="math inline">\(x_{S+u}(f\land
u=1)\)</span> and <span class="math inline">\(x_{S+u}(f\land
u=0)\)</span>. One can verify that <span class="math inline">\(f'=x_{S+u}(f\land u=1)+x_{S+u}(f\land
u=0)\)</span>. Then it remains to show <span class="math inline">\(\langle x_S(f'),
x_S(f)\rangle=\|x_S(f)\|^2\)</span>, which immediately follows from
3.</li>
</ol>
</div>
<p></p>
Then write <span class="math inline">\(x_u=x_{\set{u}}(1)\)</span>. The
follwing “SDP” is a relaxation of sparsest cut.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   \frac{\sum_{u &lt; v}c_{u,v}\|x_u-x_v\|^2}{\sum_{u
&lt; v}D_{u,v}\|x_u-x_v\|^2}\\
s.t.&amp;   &amp;   \sum_{u &lt; v}D_{u,v}\|x_u-x_v\|^2&amp;\geq 0\\
    &amp;   &amp;   x\in \mathop{\mathrm{Las}}_r(V)&amp;
\end{aligned}
\end{equation*}\]</span>
<p></p>
Scaling every <span class="math inline">\(x_S(f)\)</span> by a factor of
the square root of the objective’s denominator gives us a real SDP.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   \sum_{u &lt; v}c_{u,v}\|x_u-x_v\|^2\\
s.t.&amp;   &amp;   \sum_{u &lt; v}D_{u,v}\|x_u-x_v\|^2&amp;= 1\\
    &amp;   &amp;   x\in
\mathop{\mathrm{Las}}_r(V),\|x_\emptyset\|^2&amp;&gt;0
\end{aligned}
\end{equation*}\]</span>
<p></p>
The rounding method is too complicated, so it won’t be covered here.
<h2 data-number="5.2" id="matching"><span class="header-section-number">5.2</span> Matching</h2>
<p></p>
This application can be found in section 3.3 of <a href="https://sites.math.washington.edu/~rothvoss/lecturenotes/lasserresurvey.pdf"><code>useful_link[1]</code></a>.
We consider the maximum matching IP in non-bipartite graphs. Let <span class="math inline">\(K=\set{x\in \R_{\geq 0}^n |  \sum_{e\in
\delta(v)}x_e\geq 1 \; \forall v\in V}\)</span> be the polytope and
consider <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span>. In the notes
Rothvoss shows the following lemma.
<div id="matchinggap1" class="theorem-environment Lemma" data-index="9" type="Lemma">
<span class="theorem-header"><span class="type">Lemma</span><span class="index">9</span></span>
<p></p>
<span class="math inline">\(\mathop{\mathrm{Las}}_t^{proj}(K)\subseteq
(1+\frac{1}{2t})\cdot\mathop{\mathrm{conv}}(K\cap \set{0,1}^n)\)</span>.
</div>
<div class="theorem-environment Proof" type="Proof">
<span class="theorem-header"><span class="type">Proof</span></span>
<p></p>
Let <span class="math inline">\(y\in
\mathop{\mathrm{Las}}_t(K)\)</span>. It suffices to show that <span class="math inline">\(\sum_{e\in E[U]} y_e\leq
(1+\frac{1}{2t})k\)</span> for all <span class="math inline">\(|U|=2k+1\)</span>, since <span class="math inline">\(\set{x\in K| \text{$x$ satisfies odd
constraints}}\)</span> is the matching polytope. When <span class="math inline">\(k&gt;t\)</span>, the degree constraints imply that
<span class="math inline">\(\sum_{e\in E[U]} y_e\leq k+\frac{1}{2} \leq
(1+\frac{1}{2t})k\)</span>. Now consider the case <span class="math inline">\(k\leq t\)</span>. Note that for fixed <span class="math inline">\(U\)</span>, any <span class="math inline">\(I\subset E[U]\)</span> of size <span class="math inline">\(|I|&gt; k\)</span> has <span class="math inline">\(y_I=0\)</span>, since it is impossible to find a
matching in <span class="math inline">\(U\)</span> covering more that
<span class="math inline">\(k\)</span> vertices. Then by <a href="#conv" title="Lemma 4">Lemma 4</a> <span class="math inline">\(y\)</span> can
be represented as a convex combination of solutions <span class="math inline">\(z\in \mathop{\mathrm{Las}}_0(K)\)</span> in which
<span class="math inline">\(z_e\in \set{0,1}\)</span> for all <span class="math inline">\(e\in E[U]\)</span>. The convex combination implies
that <span class="math inline">\(\sum_{e\in E[U]} y_e\leq k\)</span>
when <span class="math inline">\(k\leq t\)</span>.
</div>
<p></p>
However, one can see that <a href="#matchinggap1" title="Lemma 9">Lemma
9</a> is not tight. <span class="math inline">\(\mathop{\mathrm{Las}}_0^{proj}(K)\)</span> should
be contained in <span class="math inline">\((1+\frac{1}{2})\cdot\mathop{\mathrm{conv}}(K\cap
\set{0,1}^n)\)</span> and <span class="math inline">\(\mathop{\mathrm{Las}}_n^{proj}(K)\)</span> should
be exactly the integer hull. Can we prove a slightly better gap that
matches observations at <span class="math inline">\(\mathop{\mathrm{Las}}_0\)</span> and <span class="math inline">\(\mathop{\mathrm{Las}}_n\)</span>? The later part
of the proof in fact shows that <span class="math inline">\(y\in
\mathop{\mathrm{Las}}_t(K)\)</span> satisfies all odd constraints with
<span class="math inline">\(|U|\leq 2t+1\)</span>. Consider an odd cycle
with <span class="math inline">\(2t+3\)</span> vertices. <span class="math inline">\((1/2,\ldots,1/2)^T\in \R^{2t+3}\)</span> is a
feasible solution in <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span> and proves a
tight lowerbound of <span class="math inline">\(k+1/2\)</span>.
<h1 data-number="6" id="questions"><span class="header-section-number">6</span> Questions</h1>
<h2 data-number="6.1" id="replace-m_tellysucceq-0-with-mathopmathrmlas_tprojyin-k"><span class="header-section-number">6.1</span> Replace <span class="math inline">\(M_t^\ell(y)\succeq 0\)</span> with <span class="math inline">\(\mathop{\mathrm{Las}}_t^{proj}(y)\in
K\)</span></h2>
<p></p>
<del>I don’t see any proof relying on the psdness of slack moment
matrices…</del>
<p></p>
It turns out that problems occur in the proof of <a href="#conv" title="Lemma 4">Lemma 4</a>. If <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span> is defined as
<span class="math inline">\(\set{y|M_t(y)\succeq 0, y^{proj}\in
K}\)</span>, then we cannot guarantee <span class="math inline">\(z^{(1)},z^{(2)}\in K\)</span>. Without <a href="#conv" title="Lemma 4">Lemma 4</a>, <span class="math inline">\(\mathop{\mathrm{Las}}_n^{proj}(K)\)</span> may not
be exactly <span class="math inline">\(K\cap \set{0,1}^n\)</span> and
the hierarchy seems less interesting? But an alternative formulation
(see <a href="#sparsest-cut">Sparsest cut</a>, which entirely ignore the
slack moment matrices) still allows good rounding even without <a href="#conv" title="Lemma 4">Lemma 4</a>. Generally speaking, if the
psdness of slack moment matrices is neglected, then we won’t have <a href="https://en.wikipedia.org/wiki/Law_of_total_probability">Law of
total probability</a>(<a href="#conv" title="Lemma 4">Lemma 4</a>);
However, we still have “finite additivity property of probability
measures”(<a href="#pseudoPr" title="Lemma 8">Lemma 8</a> (3)).
<h2 data-number="6.2" id="separation-oracle-for-implicit-k"><span class="header-section-number">6.2</span> Separation Oracle for Implicit
<span class="math inline">\(K\)</span></h2>
<p></p>
Sometimes <span class="math inline">\(K\)</span> is given in a compact
form. For example, consider finding matroid cogirth.
<p></p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
\min&amp;   &amp;   \sum_{e\in E} x_e&amp;  &amp;   &amp;\\
s.t.&amp;   &amp;   \sum_{e\in B} x_e&amp;\geq 1 &amp;  &amp;\forall
\text{ base $B$}\\
    &amp;   &amp;                 x_e&amp;\geq 0 &amp;  &amp;\forall
e\in E
\end{aligned}
\end{equation*}\]</span>
<p></p>
If <span class="math inline">\(K\)</span> is only accessable through a
separation oracle, is it possible to optimize over <span class="math inline">\(\mathop{\mathrm{Las}}_t(K)\)</span> in polynomial
time for constant <span class="math inline">\(t\)</span>?
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-laurent_comparison_2003" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">M.
Laurent, A <span>Comparison</span> of the
<span>Sherali</span>-<span>Adams</span>,
<span>Lovász</span>-<span>Schrijver</span>, and <span>Lasserre</span>
<span>Relaxations</span> for 0–1 <span>Programming</span>,
<em>Mathematics of Operations Research</em>. 28 (2003) 470–496 <a href="https://doi.org/10.1287/moor.28.3.470.16391">10.1287/moor.28.3.470.16391</a>.</div>
</div>
<div id="ref-schrijver_polyhedral_1986" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A.
Schrijver, Polyhedral proof methods in combinatorial optimization,
<em>Discrete Applied Mathematics</em>. 14 (1986) 111–133 <a href="https://doi.org/10.1016/0166-218X(86)90056-9">10.1016/0166-218X(86)90056-9</a>.</div>
</div>
<div id="ref-karlin_integrality_2011" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">A.R. Karlin, C. Mathieu, C.T. Nguyen,
Integrality <span>Gaps</span> of <span>Linear</span> and
<span>Semi</span>-<span>Definite</span> <span>Programming</span>
<span>Relaxations</span> for <span>Knapsack</span>, in: <em>Integer
<span>Programming</span> and <span>Combinatoral</span>
<span>Optimization</span></em>, Springer, Berlin, Heidelberg, 2011: pp.
301–314 <a href="https://doi.org/10.1007/978-3-642-20807-2_24">10.1007/978-3-642-20807-2_24</a>.</div>
</div>
<div id="ref-guruswami_approximating_2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">V.
Guruswami, A.K. Sinop, Approximating non-uniform sparsest cut via
generalized spectra, in: <em>Proceedings of the Twenty-Fourth Annual
<span>ACM</span>-<span>SIAM</span> Symposium on <span>Discrete</span>
Algorithms</em>, <span>Society for Industrial and Applied
Mathematics</span>, USA, 2013: pp. 295–305.</div>
</div>
</div>
    </section>
</article>

        </main>

        <footer class="no-print">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            <a href="https://github.com/congyu711/Hakyllsite">Source</a> on Github.
            License <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 </a> <img src="../../images/ccbysa.png" alt="Creative Commons License" style="height: 12px; vertical-align: baseline;">

        </footer>
    </div>
</body>

</html>